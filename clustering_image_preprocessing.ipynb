{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# set working directory and read in dataframes of image file names\n",
    "wkdir=Path(\".\")\n",
    "sam_df=pd.read_csv(wkdir/\"preprocessed_data\"/\"Sam_aiueoe_100.csv\")\n",
    "yc_df=pd.read_csv(wkdir/\"preprocessed_data\"/\"Yen-chen_aiueoe_100.csv\")\n",
    "\n",
    "# create vowel category column\n",
    "sam_df[\"Vowel\"]=[re.findall(r\"(^\\D+)\", label)[0] for label in sam_df[\"Label\"]]\n",
    "yc_df[\"Vowel\"]=[re.findall(r\"(^\\D+)\", label)[0] for label in yc_df[\"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# create folders to store preprocessed images\n",
    "try:\n",
    "    os.makedirs((wkdir/\"sam_frames_preprocessed\"))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs((wkdir/\"yenchen_frames_preprocessed\"))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# preprocess images by converting to grayscale, crop, and resize, then save to a different folder\n",
    "# images are cropped to 700*480 pixels, then downsized to 140*96 pixels\n",
    "for thisimagepath in sam_df[\"Label\"]:\n",
    "    image=cv2.imread(filename=str(wkdir/\"sam_frames\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED)\n",
    "    gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cropped_gray_image=gray_image[82:562, 211:911].copy()\n",
    "    resized_cropped_gray_image=cv2.resize(src=cropped_gray_image, dsize=(140, 96), interpolation=cv2.INTER_AREA)\n",
    "    cv2.imwrite(filename=str(wkdir/\"sam_frames_preprocessed\"/thisimagepath), img=resized_cropped_gray_image)\n",
    "\n",
    "for thisimagepath in yc_df[\"Label\"]:\n",
    "    image=cv2.imread(filename=str(wkdir/\"yenchen_frames\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED)\n",
    "    gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cropped_gray_image=gray_image[82:562, 211:911].copy()\n",
    "    resized_cropped_gray_image=cv2.resize(src=cropped_gray_image, dsize=(140, 96), interpolation=cv2.INTER_AREA)\n",
    "    cv2.imwrite(filename=str(wkdir/\"yenchen_frames_preprocessed\"/thisimagepath), img=resized_cropped_gray_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import Bunch\n",
    "\n",
    "# transform preprocessed image data and corresponding vowel labels into a dictionary\n",
    "# this dictionary is in a format that can be passed to classification and clustering algorithms\n",
    "image_list=list()\n",
    "flat_image_list=list()\n",
    "vowel_category_list=list()\n",
    "for irow in range(sam_df.shape[0]):\n",
    "    thisimagepath=sam_df[\"Label\"][irow]\n",
    "    image_list.append(cv2.imread(filename=str(wkdir/\"sam_frames_preprocessed\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED))\n",
    "    flat_image_list.append(cv2.imread(filename=str(wkdir/\"sam_frames_preprocessed\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED).flatten())\n",
    "    vowel_category_list.append(sam_df[\"Vowel\"][irow])\n",
    "sam_data=Bunch(flat_images=np.array(flat_image_list), target=np.array(vowel_category_list), images=np.array(image_list))\n",
    "\n",
    "image_list=list()\n",
    "flat_image_list=list()\n",
    "vowel_category_list=list()\n",
    "for irow in range(yc_df.shape[0]):\n",
    "    thisimagepath=yc_df[\"Label\"][irow]\n",
    "    image_list.append(cv2.imread(filename=str(wkdir/\"yenchen_frames_preprocessed\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED))\n",
    "    flat_image_list.append(cv2.imread(filename=str(wkdir/\"yenchen_frames_preprocessed\"/thisimagepath), flags=cv2.IMREAD_UNCHANGED).flatten())\n",
    "    vowel_category_list.append(yc_df[\"Vowel\"][irow])\n",
    "yc_data=Bunch(flat_images=np.array(flat_image_list), target=np.array(vowel_category_list), images=np.array(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save data objects as pickle files\n",
    "with open((wkdir/\"preprocessed_data\"/\"sam_data.pkl\"), \"wb\") as output_file:\n",
    "    pickle.dump(obj=sam_data, file=output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open((wkdir/\"preprocessed_data\"/\"yc_data.pkl\"), \"wb\") as output_file:\n",
    "    pickle.dump(obj=yc_data, file=output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any further analysis, read in pickled data using the code below\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open((wkdir/\"preprocessed_data\"/\"sam_data.pkl\"), \"rb\") as input_file:\n",
    "    sam_data=pickle.load(input_file)\n",
    "with open((wkdir/\"preprocessed_data\"/\"yc_data.pkl\"), \"rb\") as input_file:\n",
    "    yc_data=pickle.load(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
